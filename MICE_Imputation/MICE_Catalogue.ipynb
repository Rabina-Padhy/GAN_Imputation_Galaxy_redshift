{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "sns.set_style('whitegrid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import time\n",
    "from tqdm import tqdm\n",
    "import lightgbm as lgb\n",
    "from pandas.api.types import is_categorical_dtype, is_numeric_dtype\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "class BaseMICE:\n",
    "    \"\"\"Base class for the MICE implementation.\"\"\"\n",
    "    \n",
    "    def __init__(self, max_iter=10, previous_loss = 0):\n",
    "        self.max_iter = max_iter\n",
    "        self.previous_loss = previous_loss\n",
    "    \n",
    "    def fill_missing_values(self, df):\n",
    "        \"\"\"Fills the missing values of a pandas DataFrame.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        df : pandas.DataFrame\n",
    "            Input data with missing values (nans).\n",
    "            \n",
    "        Returns\n",
    "        -------\n",
    "        pandas.DataFrame\n",
    "            DataFrame with imputed missing values.\n",
    "        \"\"\"\n",
    "        nan_ids = np.argwhere(df.isna().values).tolist()\n",
    "        df_imputed = self.impute_initial_mean_or_mode(df)\n",
    "        iter_results = []\n",
    "        for iter in range(self.max_iter):\n",
    "            df_imputed = self.transform(df_imputed, nan_ids, iter)\n",
    "        return df_imputed\n",
    "    \n",
    "    def benchmark(self, df_original, df_missing, drop_columns_loss=None):\n",
    "        \"\"\"Benchmarks the fill method for missing values.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        df_original : pandas.DataFrame\n",
    "            Original data.\n",
    "        df_missing : pandas.DataFrame\n",
    "            Input data with missing values (nans).\n",
    "        drop_columns_loss : list, optional\n",
    "            Drop columns in the result DataFrame when \n",
    "            computing the loss.\n",
    "            \n",
    "        Returns\n",
    "        -------\n",
    "        pandas.DataFrame\n",
    "            DataFrame with imputed missing values.\n",
    "        \"\"\"\n",
    "        columns_missing = df_missing.isna().sum()\n",
    "        columns_missing = columns_missing[columns_missing > 0]\n",
    "        nan_ids = np.argwhere(df_missing.isna().values).tolist()\n",
    "        df_imputed = self.impute_initial_mean_or_mode(df_missing)\n",
    "        self.df_mean = df_imputed.copy()\n",
    "        \n",
    "        iter_results = []\n",
    "        for iter in range(self.max_iter):\n",
    "            time_start = time()\n",
    "            df_imputed = self.transform(df_imputed, columns_missing, nan_ids, iter)\n",
    "            time_stop = time() - time_start\n",
    "            if drop_columns_loss:\n",
    "                loss = self.compute_loss(df_original.drop(columns=drop_columns_loss, axis=1), \n",
    "                                         df_imputed.drop(columns=drop_columns_loss, axis=1))\n",
    "            else:\n",
    "                loss = self.compute_loss(df_original, df_imputed)\n",
    "            if self.previous_loss == 0:\n",
    "                self.previous_loss = loss\n",
    "                df_imputed.to_csv(\"imputed_df.csv\", index = False)\n",
    "            elif self.previous_loss < loss:\n",
    "                self.previous_loss = loss\n",
    "                df_imputed.to_csv(\"imputed_df.csv\", index = False)\n",
    "            iter_results.append({\n",
    "                \"iter\": iter,\n",
    "                \"time_seconds\": time_stop, \n",
    "                \"loss\": loss\n",
    "            })\n",
    "        return iter_results\n",
    "    \n",
    "    def benchmark_mean_loss(self, df_original, df_missing, drop_columns_loss=None):\n",
    "        \"\"\"Computes the same iterations as benchmark() but only for the of the mean imputation method.\"\"\"\n",
    "        time_start = time()\n",
    "        df_imputed = self.impute_initial_mean_or_mode(df_missing)\n",
    "        time_stop = time() - time_start\n",
    "        \n",
    "        iter_results = []\n",
    "        for iter in range(self.max_iter):\n",
    "            if drop_columns_loss:\n",
    "                loss = self.compute_loss(df_original.drop(columns=drop_columns_loss, axis=1), \n",
    "                                         df_imputed.drop(columns=drop_columns_loss, axis=1))\n",
    "            else:\n",
    "                loss = self.compute_loss(df_original, df_imputed)\n",
    "            iter_results.append({\n",
    "                \"iter\": iter,\n",
    "                \"time_seconds\": time_stop, \n",
    "                \"loss\": loss\n",
    "            })\n",
    "        return iter_results\n",
    "    \n",
    "    def get_model(self, target):\n",
    "        if is_numeric_dtype(target):\n",
    "            model = lgb.LGBMRegressor()\n",
    "        else:\n",
    "            model = lgb.LGBMClassifier()\n",
    "        return model\n",
    "    \n",
    "    def compute_loss(self, original_df, filled_df):\n",
    "        \"\"\"Computes the difference between the original and filled DataFrames.\"\"\"\n",
    "        return mean_squared_error(original_df, filled_df)\n",
    "    \n",
    "    def impute_initial_mean_or_mode(self, df):\n",
    "        df_new = df.copy()\n",
    "        for column in df:\n",
    "            if is_numeric_dtype(df[column]):\n",
    "                df_new[column] = df_new[column].fillna(df_new[column].mean())\n",
    "            else:\n",
    "                df_new[column] = df_new[column].fillna(df_new[column].mode())\n",
    "        return df_new\n",
    "    \n",
    "    def transform(self, df, nan_ids):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VanilaMICE(BaseMICE):\n",
    "    \"\"\"MICE implementation using value by value imputation.\"\"\"\n",
    "    \n",
    "    method_name = \"Vanila MICE\"\n",
    "    \n",
    "    def transform(self, df: pd.DataFrame, columns_missing: list, nan_ids: list, iter_id: int):\n",
    "        random_ids = np.random.permutation(len(nan_ids)).tolist()\n",
    "        for id in tqdm(random_ids, desc=f\"{self.method_name}: Iter {iter_id + 1} / {self.max_iter}\", position=0):\n",
    "            # Setup data\n",
    "            row_id, col_id = nan_ids[id]\n",
    "            target_column_name = df.columns[col_id]\n",
    "            X = df.drop(columns=[target_column_name], axis=1)\n",
    "            X = pd.get_dummies(X, drop_first=True)\n",
    "            y = df[target_column_name]\n",
    "            \n",
    "            # Fit model\n",
    "            model = self.get_model(y).fit(X.drop(index=row_id), y.drop(index=row_id))\n",
    "            \n",
    "            # Predict value\n",
    "            df.iloc[row_id, col_id] = model.predict(X.iloc[row_id:row_id + 1, :])\n",
    "        return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Vanila MICE: Iter 1 / 10:   0%|                                                                | 0/666 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1311, 10)\n",
      "(1311, 9)\n",
      "(1311, 1)\n",
      "(1311, 10)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Vanila MICE: Iter 1 / 10: 100%|██████████████████████████████████████████████████████| 666/666 [01:55<00:00,  5.75it/s]\n",
      "Vanila MICE: Iter 2 / 10: 100%|██████████████████████████████████████████████████████| 666/666 [01:56<00:00,  5.71it/s]\n",
      "Vanila MICE: Iter 3 / 10: 100%|██████████████████████████████████████████████████████| 666/666 [01:57<00:00,  5.65it/s]\n",
      "Vanila MICE: Iter 4 / 10: 100%|██████████████████████████████████████████████████████| 666/666 [01:57<00:00,  5.65it/s]\n",
      "Vanila MICE: Iter 5 / 10: 100%|██████████████████████████████████████████████████████| 666/666 [01:57<00:00,  5.66it/s]\n",
      "Vanila MICE: Iter 6 / 10: 100%|██████████████████████████████████████████████████████| 666/666 [02:01<00:00,  5.47it/s]\n",
      "Vanila MICE: Iter 7 / 10: 100%|██████████████████████████████████████████████████████| 666/666 [01:56<00:00,  5.73it/s]\n",
      "Vanila MICE: Iter 8 / 10: 100%|██████████████████████████████████████████████████████| 666/666 [01:57<00:00,  5.69it/s]\n",
      "Vanila MICE: Iter 9 / 10: 100%|██████████████████████████████████████████████████████| 666/666 [01:54<00:00,  5.81it/s]\n",
      "Vanila MICE: Iter 10 / 10: 100%|█████████████████████████████████████████████████████| 666/666 [01:55<00:00,  5.75it/s]\n"
     ]
    }
   ],
   "source": [
    "#dataNames = ['vals_test_df', 'vals_test_df_test_type1', 'vals_test_df_test_type2']\n",
    "\n",
    "dataNames = ['vals_test_df_test_type2']\n",
    "for dataName in dataNames:\n",
    "    file_name = dataName+\".csv\"\n",
    "    if dataName =='vals_test_df':\n",
    "        train_dataName = 'vals_train_df'+'.csv'\n",
    "    elif dataName =='vals_test_df_test_type1':\n",
    "        train_dataName = 'vals_train_df_test_type1'+'.csv'\n",
    "    elif dataName =='vals_test_df_test_type2':\n",
    "        train_dataName = 'vals_train_df_test_type2'+'.csv'\n",
    "    load_test_cataglogue_data = pd.read_csv(file_name, header = None, skiprows=1 )\n",
    "    load_train_cataglogue_data = pd.read_csv(train_dataName, header = None, skiprows=1 )\n",
    "    load_cataglogue_data = pd.concat([load_train_cataglogue_data, load_test_cataglogue_data], ignore_index= True)\n",
    "    print(load_cataglogue_data.shape)\n",
    "    load_cataglogue_data.to_csv(\"original_catalogue.csv\", index = False)\n",
    "    #catalogue_features_df = load_cataglogue_data[['1', '2', '3', '4', '5', '6', '7', '8', '9']]\n",
    "    catalogue_features_df = pd.read_csv(\"original_catalogue.csv\", usecols = [*range(1, 10)], header=None, skiprows=1)\n",
    "    print(catalogue_features_df.shape)\n",
    "    #catalogue_target_df = load_cataglogue_data[['0']]\n",
    "    catalogue_target_df = pd.read_csv(\"original_catalogue.csv\", usecols = [0], header=None, skiprows=1)\n",
    "    print(catalogue_target_df.shape)\n",
    "    \n",
    "    #show_sample(catalogue_features_df, catalogue_target_df)\n",
    "    max_iter = 10\n",
    "    results = []\n",
    "    # setup data\n",
    "    features_df, targets_df = catalogue_features_df, catalogue_target_df\n",
    "    original_df = pd.concat([catalogue_features_df,catalogue_target_df], axis=1)\n",
    "\n",
    "    #print(original_df.shape)\n",
    "    missing_test_file_name = dataName + \"_generated.csv\"\n",
    "    missing_test_df_x = pd.read_csv(missing_test_file_name, header= None, skiprows = 1,  usecols = [*range(1, 10)])\n",
    "    missing_test_df_y = pd.read_csv(missing_test_file_name, header= None, skiprows = 1,  usecols = [0])\n",
    "    missing_test_df = pd.concat([missing_test_df_x,missing_test_df_y], axis= 1)\n",
    "    load_train_cataglogue_data_x = pd.read_csv(train_dataName, header = None, skiprows=1,  usecols = [*range(1, 10)] )\n",
    "    load_train_cataglogue_data_y = pd.read_csv(train_dataName, header = None, skiprows=1,  usecols = [0] )\n",
    "    load_train_cataglogue_data = pd.concat([load_train_cataglogue_data_x, load_train_cataglogue_data_y], axis= 1)\n",
    "    nans_df = pd.concat([load_train_cataglogue_data,missing_test_df], ignore_index= True)\n",
    "    print(nans_df.shape)\n",
    "\n",
    "      \n",
    "    \n",
    "    # Compute mean squared error loss\n",
    "    mean_iters = BaseMICE(max_iter).benchmark_mean_loss(original_df, nans_df, drop_columns_loss=None)\n",
    "    vanila_iters = VanilaMICE(max_iter).benchmark(original_df, nans_df, drop_columns_loss=None)\n",
    "\n",
    "\n",
    "    results.append({\n",
    "        \"dataset\": dataName,\n",
    "        \"results\": {\n",
    "            \"mean\": mean_iters,\n",
    "            \"vanila_mice\": vanila_iters\n",
    "        }\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'vals_test_df_test_type2':                    dataset       method  num_iters  total_time      loss\n",
      "0  vals_test_df_test_type2         mean         10           0  0.042994\n",
      "1  vals_test_df_test_type2  vanila_mice         10        1171  0.010170}\n"
     ]
    }
   ],
   "source": [
    "results_by_dataset = {}\n",
    "for dataset in results:\n",
    "    name = dataset['dataset']\n",
    "    \n",
    "    methods = []\n",
    "    for method in dataset['results']:\n",
    "        num_iters = sum([1 for iter in dataset['results'][method]])\n",
    "        total_time = sum([iter[\"time_seconds\"] for iter in dataset['results'][method]])\n",
    "        loss = np.mean([iter[\"loss\"] for iter in dataset['results'][method]])\n",
    "        methods.append([name, method, num_iters, round(total_time), loss])\n",
    "    results_by_dataset[name] = pd.DataFrame(methods, columns=['dataset', 'method', 'num_iters', 'total_time', 'loss'])\n",
    "\n",
    "print(results_by_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataName = 'vals_test_df'\n",
    "imputed_df_x = pd.read_csv(\"imputed_df.csv\", header= None, skiprows = 1, usecols = [*range(0, 9)])\n",
    "imputed_df_y = pd.read_csv(\"imputed_df.csv\", header= None, skiprows = 1, usecols = [9])\n",
    "imputed_df = pd.concat([imputed_df_y, imputed_df_x], axis=1)\n",
    "if dataName ==  'vals_test_df':\n",
    "    MICE_imputed_df = imputed_df.iloc[918:1311, :]\n",
    "elif dataName ==  'vals_test_df_test_type1':\n",
    "    MICE_imputed_df = imputed_df.iloc[495:1311, :]\n",
    "elif dataName ==  'vals_test_df_test_type2':\n",
    "    MICE_imputed_df = imputed_df.iloc[816:1311, :]\n",
    "\n",
    "MICE_imputed_df.columns = ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9']\n",
    "MICE_imputed_df.to_csv(\"MICE_imputated_catalogueData1.csv\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1311, 10)\n",
      "(1311, 10)\n",
      "          1          2          3          4         5         6         7  \\\n",
      "0 -0.073577  12.072797  13.743570  10.600228  4.976427 -2.036147 -1.714266   \n",
      "1 -0.130407  -0.582912  -0.560864  -0.312900 -0.421307  1.085241  1.102540   \n",
      "2 -0.100489  -0.265857  -0.325232  -0.180335 -0.423785  1.479124  1.510337   \n",
      "3 -0.095435  -0.432822  -0.343848  -0.151197 -0.174168  0.280763  0.246145   \n",
      "4 -0.076036  -0.036002   0.054097  -0.050561  0.157918 -0.322421 -0.384505   \n",
      "\n",
      "          8         9         0  \n",
      "0 -1.763495 -1.725079  0.169560  \n",
      "1  1.155726  1.171365  0.538615  \n",
      "2  1.341343  1.251914  0.909825  \n",
      "3  0.182350  0.246956  0.271896  \n",
      "4 -0.460758 -0.515073  0.237271  \n",
      "          1          2          3          4         5         6         7  \\\n",
      "0 -0.073577  12.072797  13.743570  10.600228  4.976427 -2.036147 -1.714266   \n",
      "1 -0.130407  -0.582912  -0.560864  -0.312900 -0.421307  1.085241  1.102540   \n",
      "2 -0.100489  -0.265857  -0.325232  -0.180335 -0.423785  1.479124  1.510337   \n",
      "3 -0.095435  -0.432822  -0.343848  -0.151197 -0.174168  0.280763  0.246145   \n",
      "4 -0.076036  -0.036002   0.054097  -0.050561  0.157918 -0.322421 -0.384505   \n",
      "\n",
      "          8         9         0  \n",
      "0 -1.763495 -1.725079  0.169560  \n",
      "1  1.155726  1.171365  0.538615  \n",
      "2  1.341343  1.251914  0.909825  \n",
      "3  0.182350  0.246956  0.271896  \n",
      "4 -0.460758 -0.515073  0.237271  \n",
      "(1311, 1)\n"
     ]
    }
   ],
   "source": [
    "features_df, targets_df = catalogue_features_df, catalogue_target_df\n",
    "original_df = pd.concat([catalogue_features_df,catalogue_target_df], axis=1)\n",
    "print(original_df.shape)\n",
    "missing_test_df_x = pd.read_csv(\"vals_test_df_generated.csv\", header= None, skiprows = 1,  usecols = [*range(1, 10)])\n",
    "missing_test_df_y = pd.read_csv(\"vals_test_df_generated.csv\", header= None, skiprows = 1,  usecols = [0])\n",
    "missing_test_df = pd.concat([missing_test_df_x,missing_test_df_y], axis= 1)\n",
    "load_train_cataglogue_data_x = pd.read_csv(\"vals_train_df.csv\", header = None, skiprows=1,  usecols = [*range(1, 10)] )\n",
    "load_train_cataglogue_data_y = pd.read_csv(\"vals_train_df.csv\", header = None, skiprows=1,  usecols = [0] )\n",
    "load_train_cataglogue_data = pd.concat([load_train_cataglogue_data_x, load_train_cataglogue_data_y], axis= 1)\n",
    "nans_df = pd.concat([load_train_cataglogue_data,missing_test_df])\n",
    "#nans_df = pd.concat([nans_df, catalogue_target_df], axis = 1)\n",
    "print(nans_df.shape)\n",
    "print(original_df.head(5))\n",
    "print(nans_df.head(5))\n",
    "print(catalogue_target_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Vanila MICE: Iter 1 / 5:   0%|                                                                 | 0/686 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1311, 10)\n",
      "(1311, 10)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Vanila MICE: Iter 1 / 5: 100%|███████████████████████████████████████████████████████| 686/686 [03:24<00:00,  3.36it/s]\n",
      "Vanila MICE: Iter 2 / 5: 100%|███████████████████████████████████████████████████████| 686/686 [03:23<00:00,  3.37it/s]\n",
      "Vanila MICE: Iter 3 / 5: 100%|███████████████████████████████████████████████████████| 686/686 [03:29<00:00,  3.28it/s]\n",
      "Vanila MICE: Iter 4 / 5: 100%|███████████████████████████████████████████████████████| 686/686 [03:15<00:00,  3.50it/s]\n",
      "Vanila MICE: Iter 5 / 5: 100%|███████████████████████████████████████████████████████| 686/686 [03:32<00:00,  3.22it/s]\n"
     ]
    }
   ],
   "source": [
    "from time import time\n",
    "from tqdm import tqdm\n",
    "import lightgbm as lgb\n",
    "from pandas.api.types import is_categorical_dtype, is_numeric_dtype\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "class BaseMICE:\n",
    "    \"\"\"Base class for the MICE implementation.\"\"\"\n",
    "    \n",
    "    def __init__(self, max_iter=10):\n",
    "        self.max_iter = max_iter\n",
    "    \n",
    "    def fill_missing_values(self, df):\n",
    "        \"\"\"Fills the missing values of a pandas DataFrame.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        df : pandas.DataFrame\n",
    "            Input data with missing values (nans).\n",
    "            \n",
    "        Returns\n",
    "        -------\n",
    "        pandas.DataFrame\n",
    "            DataFrame with imputed missing values.\n",
    "        \"\"\"\n",
    "        nan_ids = np.argwhere(df.isna().values).tolist()\n",
    "        df_imputed = self.impute_initial_mean_or_mode(df)\n",
    "        iter_results = []\n",
    "        for iter in range(self.max_iter):\n",
    "            df_imputed = self.transform(df_imputed, nan_ids, iter)\n",
    "        return df_imputed\n",
    "    \n",
    "    def benchmark(self, df_original, df_missing, drop_columns_loss=None):\n",
    "        \"\"\"Benchmarks the fill method for missing values.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        df_original : pandas.DataFrame\n",
    "            Original data.\n",
    "        df_missing : pandas.DataFrame\n",
    "            Input data with missing values (nans).\n",
    "        drop_columns_loss : list, optional\n",
    "            Drop columns in the result DataFrame when \n",
    "            computing the loss.\n",
    "            \n",
    "        Returns\n",
    "        -------\n",
    "        pandas.DataFrame\n",
    "            DataFrame with imputed missing values.\n",
    "        \"\"\"\n",
    "        columns_missing = df_missing.isna().sum()\n",
    "        columns_missing = columns_missing[columns_missing > 0]\n",
    "        nan_ids = np.argwhere(df_missing.isna().values).tolist()\n",
    "        df_imputed = self.impute_initial_mean_or_mode(df_missing)\n",
    "        self.df_mean = df_imputed.copy()\n",
    "        \n",
    "        iter_results = []\n",
    "        for iter in range(self.max_iter):\n",
    "            time_start = time()\n",
    "            #print(columns_missing)\n",
    "            #print(nan_ids)\n",
    "            #print(df_imputed.shape)\n",
    "            df_imputed = self.transform(df_imputed, columns_missing, nan_ids, iter)\n",
    "            time_stop = time() - time_start\n",
    "            previous_loss = 0\n",
    "            if drop_columns_loss:\n",
    "                loss = self.compute_loss(df_original.drop(columns=drop_columns_loss, axis=1), \n",
    "                                         df_imputed.drop(columns=drop_columns_loss, axis=1))\n",
    "            else:\n",
    "                loss = self.compute_loss(df_original, df_imputed)\n",
    "            if previous_loss == 0:\n",
    "                previous_loss = loss\n",
    "                df_imputed.to_csv(\"imputed_df.csv\", index = False)\n",
    "            elif previous_loss < loss:\n",
    "                previous_loss = loss\n",
    "                df_imputed.to_csv(\"imputed_df.csv\", index = False)\n",
    "            iter_results.append({\n",
    "                \"iter\": iter,\n",
    "                \"time_seconds\": time_stop, \n",
    "                \"loss\": loss\n",
    "            })\n",
    "        return iter_results\n",
    "    \n",
    "    def benchmark_mean_loss(self, df_original, df_missing, drop_columns_loss=None):\n",
    "        \"\"\"Computes the same iterations as benchmark() but only for the of the mean imputation method.\"\"\"\n",
    "        time_start = time()\n",
    "        df_imputed = self.impute_initial_mean_or_mode(df_missing)\n",
    "        time_stop = time() - time_start\n",
    "        \n",
    "        iter_results = []\n",
    "        for iter in range(self.max_iter):\n",
    "            if drop_columns_loss:\n",
    "                loss = self.compute_loss(df_original.drop(columns=drop_columns_loss, axis=1), \n",
    "                                         df_imputed.drop(columns=drop_columns_loss, axis=1))\n",
    "            else:\n",
    "                loss = self.compute_loss(df_original, df_imputed)\n",
    "            iter_results.append({\n",
    "                \"iter\": iter,\n",
    "                \"time_seconds\": time_stop, \n",
    "                \"loss\": loss\n",
    "            })\n",
    "        return iter_results\n",
    "    \n",
    "    def get_model(self, target):\n",
    "        if is_numeric_dtype(target):\n",
    "            model = lgb.LGBMRegressor()\n",
    "        else:\n",
    "            model = lgb.LGBMClassifier()\n",
    "        return model\n",
    "    \n",
    "    def compute_loss(self, original_df, filled_df):\n",
    "        \"\"\"Computes the difference between the original and filled DataFrames.\"\"\"\n",
    "        return mean_squared_error(original_df, filled_df)\n",
    "    \n",
    "    def impute_initial_mean_or_mode(self, df):\n",
    "        df_new = df.copy()\n",
    "        for column in df:\n",
    "            if is_numeric_dtype(df[column]):\n",
    "                df_new[column] = df_new[column].fillna(df_new[column].mean())\n",
    "            else:\n",
    "                df_new[column] = df_new[column].fillna(df_new[column].mode())\n",
    "        return df_new\n",
    "    \n",
    "    def transform(self, df, nan_ids):\n",
    "        pass\n",
    "    \n",
    "\n",
    "class VanilaMICE(BaseMICE):\n",
    "    \"\"\"MICE implementation using value by value imputation.\"\"\"\n",
    "    \n",
    "    method_name = \"Vanila MICE\"\n",
    "    \n",
    "    def transform(self, df: pd.DataFrame, columns_missing: list, nan_ids: list, iter_id: int):\n",
    "        random_ids = np.random.permutation(len(nan_ids)).tolist()\n",
    "        #print(\"random\", random_ids)\n",
    "        for id in tqdm(random_ids, desc=f\"{self.method_name}: Iter {iter_id + 1} / {self.max_iter}\", position=0):\n",
    "            # Setup data\n",
    "            row_id, col_id = nan_ids[id]\n",
    "            target_column_name = df.columns[col_id]\n",
    "            X = df.drop(columns=[target_column_name], axis=1)\n",
    "            X = pd.get_dummies(X, drop_first=True)\n",
    "            y = df[target_column_name]\n",
    "            \n",
    "            # Fit model\n",
    "            model = self.get_model(y).fit(X.drop(index=row_id), y.drop(index=row_id))\n",
    "            \n",
    "            # Predict value\n",
    "            df.iloc[row_id, col_id] = model.predict(X.iloc[row_id:row_id + 1, :])\n",
    "        return df\n",
    "    \n",
    "    \n",
    "#show_sample(catalogue_features_df, catalogue_target_df)\n",
    "max_iter = 5\n",
    "results = []\n",
    "# setup data\n",
    "features_df, targets_df = catalogue_features_df, catalogue_target_df\n",
    "original_df = pd.concat([catalogue_features_df,catalogue_target_df], axis=1)\n",
    "#original_df.to_csv(\"original_df.csv\", index = True)\n",
    "print(original_df.shape)\n",
    "missing_test_df_x = pd.read_csv(\"vals_test_df_generated.csv\", header= None, skiprows = 1,  usecols = [*range(1, 10)])\n",
    "missing_test_df_y = pd.read_csv(\"vals_test_df_generated.csv\", header= None, skiprows = 1,  usecols = [0])\n",
    "missing_test_df = pd.concat([missing_test_df_x,missing_test_df_y], axis= 1)\n",
    "load_train_cataglogue_data_x = pd.read_csv(\"vals_train_df.csv\", header = None, skiprows=1,  usecols = [*range(1, 10)] )\n",
    "load_train_cataglogue_data_y = pd.read_csv(\"vals_train_df.csv\", header = None, skiprows=1,  usecols = [0] )\n",
    "load_train_cataglogue_data = pd.concat([load_train_cataglogue_data_x, load_train_cataglogue_data_y], axis= 1)\n",
    "nans_df = pd.concat([load_train_cataglogue_data,missing_test_df], ignore_index= True)\n",
    "#nans_df.to_csv(\"nans_df.csv\", index = True)\n",
    "#nans_df = pd.concat([nans_df, catalogue_target_df], axis = 1)\n",
    "print(nans_df.shape)\n",
    "#print(original_df.tail(5))\n",
    "#print(nans_df.tail(5))\n",
    "#print(catalogue_target_df.shape)\n",
    "#original_df, nans_df = generate_missing_values(features_df, targets_df)\n",
    "      \n",
    "    \n",
    "# Compute mean squared error loss \n",
    "mean_iters = BaseMICE(max_iter).benchmark_mean_loss(original_df, nans_df, drop_columns_loss=None)\n",
    "vanila_iters = VanilaMICE(max_iter).benchmark(original_df, nans_df, drop_columns_loss=None)\n",
    "#fast_iters = FastMICE(max_iter).benchmark(original_df, nans_df, drop_columns_loss=None)\n",
    "#slow_fast_iters = SlowFastMICE(max_iter).benchmark(original_df, nans_df, drop_columns_loss=None)\n",
    "#fast_slow_iters = FastSlowMICE(max_iter).benchmark(original_df, nans_df, drop_columns_loss=None)\n",
    "\n",
    "results.append({\n",
    "    \"dataset\": \"Catalogue\",\n",
    "    \"results\": {\n",
    "        \"mean\": mean_iters,\n",
    "        \"vanila_mice\": vanila_iters\n",
    "        #\"fast_mice\": fast_iters,\n",
    "        #\"slow_fast_mice\": slow_fast_iters,\n",
    "        #\"fast_slow_mice\": fast_slow_iters\n",
    "    }\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_by_dataset = {}\n",
    "for dataset in results:\n",
    "    name = dataset['dataset']\n",
    "    \n",
    "    methods = []\n",
    "    for method in dataset['results']:\n",
    "        num_iters = sum([1 for iter in dataset['results'][method]])\n",
    "        total_time = sum([iter[\"time_seconds\"] for iter in dataset['results'][method]])\n",
    "        loss = np.mean([iter[\"loss\"] for iter in dataset['results'][method]])\n",
    "        methods.append([name, method, num_iters, round(total_time), loss])\n",
    "    results_by_dataset[name] = pd.DataFrame(methods, columns=['dataset', 'method', 'num_iters', 'total_time', 'loss'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Catalogue':      dataset       method  num_iters  total_time      loss\n",
      "0  Catalogue         mean          5           0  0.042200\n",
      "1  Catalogue  vanila_mice          5        1026  0.005109}\n"
     ]
    }
   ],
   "source": [
    "print(results_by_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataName = 'vals_test_df'\n",
    "imputed_df_x = pd.read_csv(\"imputed_df.csv\", header= None, skiprows = 1, usecols = [*range(0, 9)])\n",
    "imputed_df_y = pd.read_csv(\"imputed_df.csv\", header= None, skiprows = 1, usecols = [9])\n",
    "imputed_df = pd.concat([imputed_df_y, imputed_df_x], axis=1)\n",
    "if dataName ==  'vals_test_df':\n",
    "    MICE_imputed_df = imputed_df.iloc[918:1311, :]\n",
    "elif dataName ==  'vals_test_df_test_type1':\n",
    "    MICE_imputed_df = imputed_df.iloc[495:1311, :]\n",
    "elif dataName ==  'vals_test_df_test_type2':\n",
    "    MICE_imputed_df = imputed_df.iloc[816:1311, :]\n",
    "\n",
    "MICE_imputed_df.columns = ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9']\n",
    "MICE_imputed_df.to_csv(\"MICE_imputated_catalogueData1.csv\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
